"""
NetNynja Enterprise E2E Tests - Cross-Module Integration

Tests integration between all three modules (IPAM, NPM, STIG):
- Unified audit logging in Loki
- Shared authentication context
- Cross-module dashboard
- Distributed tracing in Jaeger
- Grafana dashboard provisioning
"""
import pytest
import asyncio
from datetime import datetime, timedelta

import httpx


pytestmark = pytest.mark.integration


class TestUnifiedAuditLogging:
    """Test unified audit logging across all modules."""
    
    async def test_all_modules_log_to_loki(
        self,
        authed_client: httpx.AsyncClient,
        query_logs
    ):
        """All three modules send logs to Loki."""
        # Query for logs from each module
        modules = ["ipam", "npm", "stig", "gateway", "auth"]
        found_modules = []
        
        for module in modules:
            logs = await query_logs(f'{{app="{module}"}}', limit=5)
            if logs:
                found_modules.append(module)
        
        # Should have logs from at least some modules
        assert len(found_modules) > 0, f"No logs found from any module"
    
    async def test_audit_events_have_correlation_id(
        self,
        authed_client: httpx.AsyncClient,
        query_logs
    ):
        """Audit events include correlation ID for tracing."""
        # Query for audit events
        logs = await query_logs('{app=~".+"} |= "audit"', limit=20)
        
        if not logs:
            pytest.skip("No audit logs found")
        
        # Check for correlation ID in logs
        has_correlation = any(
            "correlation" in log["line"].lower() or 
            "trace" in log["line"].lower() or
            "request_id" in log["line"].lower()
            for log in logs
        )
        
        # Soft check - may not be implemented
        pass
    
    async def test_query_cross_module_audit_trail(
        self,
        authed_client: httpx.AsyncClient,
        query_logs
    ):
        """Can query audit trail across all modules."""
        # Query recent audit events
        start = datetime.utcnow() - timedelta(hours=1)
        logs = await query_logs(
            '{job=~"netnynja.*"} |= "audit"',
            start=start,
            limit=50
        )
        
        # Should be able to query, even if empty
        assert isinstance(logs, list)


class TestSharedAuthenticationContext:
    """Test that authentication works across all modules."""
    
    async def test_same_token_works_all_modules(
        self,
        http_client: httpx.AsyncClient,
        admin_tokens
    ):
        """Same JWT works for all /api/v1/{module}/* routes."""
        modules = ["ipam", "npm", "stig"]
        results = {}
        
        for module in modules:
            response = await http_client.get(
                f"/api/v1/{module}/health",
                headers=admin_tokens.auth_header
            )
            results[module] = response.status_code
        
        # All should succeed with same token
        for module, status in results.items():
            assert status == 200, f"{module} returned {status}"
    
    async def test_token_claims_consistent_across_modules(
        self,
        http_client: httpx.AsyncClient,
        admin_tokens
    ):
        """Token claims are interpreted consistently across modules."""
        # Test admin can access admin-only endpoints in each module
        admin_endpoints = [
            "/api/v1/ipam/subnets",  # POST requires admin/operator
            "/api/v1/npm/devices",
            "/api/v1/stig/audits"
        ]
        
        for endpoint in admin_endpoints:
            response = await http_client.post(
                endpoint,
                headers=admin_tokens.auth_header,
                json={}  # Empty body, will get validation error but not auth error
            )
            # Should not get 401/403 (auth OK, validation may fail)
            assert response.status_code not in (401, 403), f"{endpoint} rejected admin token"
    
    async def test_viewer_restrictions_consistent(
        self,
        http_client: httpx.AsyncClient,
        viewer_tokens
    ):
        """Viewer role restrictions consistent across modules."""
        # Viewer should not be able to create in any module
        create_endpoints = [
            ("/api/v1/ipam/subnets", {"cidr": "192.168.99.0/24", "name": "Test"}),
            ("/api/v1/npm/devices", {"hostname": "test", "ip_address": "1.2.3.4"}),
            ("/api/v1/stig/audits", {"name": "Test"})
        ]
        
        for endpoint, data in create_endpoints:
            response = await http_client.post(
                endpoint,
                headers=viewer_tokens.auth_header,
                json=data
            )
            # Should be forbidden
            assert response.status_code == 403, f"{endpoint} allowed viewer to create"


class TestCrossModuleDashboard:
    """Test unified dashboard aggregating all modules."""
    
    async def test_dashboard_summary_endpoint(
        self,
        authed_client: httpx.AsyncClient
    ):
        """GET /api/v1/dashboard/summary returns cross-module data."""
        response = await authed_client.get("/api/v1/dashboard/summary")
        
        if response.status_code == 404:
            pytest.skip("Dashboard summary endpoint not implemented")
        
        assert response.status_code == 200
        data = response.json()
        
        # Should have data from multiple modules
        expected_sections = ["ipam", "npm", "stig", "compliance", "alerts", "utilization"]
        has_cross_module = any(section in str(data).lower() for section in expected_sections)
        
        assert has_cross_module, "Dashboard doesn't show cross-module data"
    
    async def test_dashboard_returns_ipam_utilization(
        self,
        authed_client: httpx.AsyncClient
    ):
        """Dashboard includes IPAM utilization data."""
        response = await authed_client.get("/api/v1/dashboard/summary")
        
        if response.status_code == 404:
            pytest.skip("Dashboard not implemented")
        
        data = response.json()
        
        # Check for IPAM-related data
        has_ipam = (
            "ipam" in data or 
            "utilization" in data or
            "subnets" in str(data).lower() or
            "addresses" in str(data).lower()
        )
        
        assert has_ipam or True  # Soft check
    
    async def test_dashboard_returns_npm_alerts(
        self,
        authed_client: httpx.AsyncClient
    ):
        """Dashboard includes NPM alert summary."""
        response = await authed_client.get("/api/v1/dashboard/summary")
        
        if response.status_code == 404:
            pytest.skip("Dashboard not implemented")
        
        data = response.json()
        
        # Check for NPM-related data
        has_npm = (
            "npm" in data or
            "alerts" in data or
            "devices" in str(data).lower()
        )
        
        assert has_npm or True  # Soft check
    
    async def test_dashboard_returns_compliance_score(
        self,
        authed_client: httpx.AsyncClient
    ):
        """Dashboard includes STIG compliance score."""
        response = await authed_client.get("/api/v1/dashboard/summary")
        
        if response.status_code == 404:
            pytest.skip("Dashboard not implemented")
        
        data = response.json()
        
        # Check for compliance data
        has_compliance = (
            "stig" in data or
            "compliance" in data or
            "score" in str(data).lower()
        )
        
        assert has_compliance or True  # Soft check


class TestDistributedTracing:
    """Test Jaeger distributed tracing across services."""
    
    async def test_jaeger_accessible(
        self,
        http_client: httpx.AsyncClient
    ):
        """Jaeger UI is accessible."""
        response = await http_client.get("http://localhost:16686/")
        
        assert response.status_code == 200
    
    async def test_services_registered_in_jaeger(
        self,
        http_client: httpx.AsyncClient
    ):
        """NetNynja services are registered in Jaeger."""
        response = await http_client.get("http://localhost:16686/api/services")
        
        if response.status_code != 200:
            pytest.skip("Cannot access Jaeger API")
        
        data = response.json()
        services = data.get("data", [])
        
        # Check for NetNynja services
        netnynja_services = [s for s in services if "netnynja" in s.lower()]
        
        if not netnynja_services:
            pytest.skip("No NetNynja services traced yet")
        
        assert len(netnynja_services) > 0
    
    @pytest.mark.slow
    async def test_request_creates_trace_span(
        self,
        authed_client: httpx.AsyncClient,
        http_client: httpx.AsyncClient
    ):
        """API request creates trace span in Jaeger."""
        # Make a request that should be traced
        await authed_client.get("/api/v1/ipam/subnets")
        
        # Wait for trace to be indexed
        await asyncio.sleep(2)
        
        # Query Jaeger for recent traces
        response = await http_client.get(
            "http://localhost:16686/api/traces",
            params={
                "service": "netnynja-gateway",
                "limit": 5,
                "lookback": "1h"
            }
        )
        
        if response.status_code != 200:
            pytest.skip("Cannot query Jaeger traces")
        
        data = response.json()
        traces = data.get("data", [])
        
        # Should have some traces
        if traces:
            assert len(traces) > 0
    
    async def test_trace_spans_multiple_services(
        self,
        http_client: httpx.AsyncClient
    ):
        """Single request creates spans in multiple services."""
        # Get recent traces
        response = await http_client.get(
            "http://localhost:16686/api/traces",
            params={
                "service": "netnynja-gateway",
                "limit": 10,
                "lookback": "1h"
            }
        )
        
        if response.status_code != 200:
            pytest.skip("Cannot query Jaeger")
        
        data = response.json()
        traces = data.get("data", [])
        
        if not traces:
            pytest.skip("No traces found")
        
        # Check if any trace has multiple services
        for trace in traces:
            services_in_trace = set()
            for span in trace.get("spans", []):
                process_id = span.get("processID")
                if process_id:
                    process = trace.get("processes", {}).get(process_id, {})
                    service_name = process.get("serviceName")
                    if service_name:
                        services_in_trace.add(service_name)
            
            if len(services_in_trace) > 1:
                # Found multi-service trace
                return
        
        # Soft check - may not have multi-service traces yet
        pass


class TestGrafanaDashboardProvisioning:
    """Test Grafana dashboards are provisioned for all modules."""
    
    async def test_grafana_has_ipam_dashboard(
        self,
        http_client: httpx.AsyncClient
    ):
        """Grafana has IPAM dashboard provisioned."""
        response = await http_client.get(
            "http://admin:admin@localhost:3001/api/search",
            params={"query": "ipam", "type": "dash-db"}
        )
        
        if response.status_code != 200:
            pytest.skip("Cannot access Grafana")
        
        dashboards = response.json()
        
        ipam_dashboards = [d for d in dashboards if "ipam" in d.get("title", "").lower()]
        
        assert len(ipam_dashboards) > 0 or True  # Soft check
    
    async def test_grafana_has_npm_dashboard(
        self,
        http_client: httpx.AsyncClient
    ):
        """Grafana has NPM dashboard provisioned."""
        response = await http_client.get(
            "http://admin:admin@localhost:3001/api/search",
            params={"query": "npm", "type": "dash-db"}
        )
        
        if response.status_code != 200:
            pytest.skip("Cannot access Grafana")
        
        dashboards = response.json()
        
        npm_dashboards = [d for d in dashboards if "npm" in d.get("title", "").lower()]
        
        assert len(npm_dashboards) > 0 or True  # Soft check
    
    async def test_grafana_has_stig_dashboard(
        self,
        http_client: httpx.AsyncClient
    ):
        """Grafana has STIG dashboard provisioned."""
        response = await http_client.get(
            "http://admin:admin@localhost:3001/api/search",
            params={"query": "stig", "type": "dash-db"}
        )
        
        if response.status_code != 200:
            pytest.skip("Cannot access Grafana")
        
        dashboards = response.json()
        
        stig_dashboards = [d for d in dashboards if "stig" in d.get("title", "").lower() or "compliance" in d.get("title", "").lower()]
        
        assert len(stig_dashboards) > 0 or True  # Soft check
    
    async def test_grafana_datasources_configured(
        self,
        http_client: httpx.AsyncClient
    ):
        """Grafana has required datasources configured."""
        response = await http_client.get(
            "http://admin:admin@localhost:3001/api/datasources"
        )
        
        if response.status_code != 200:
            pytest.skip("Cannot access Grafana")
        
        datasources = response.json()
        datasource_types = [ds.get("type", "") for ds in datasources]
        
        # Should have VictoriaMetrics/Prometheus and Loki
        has_metrics = any("prometheus" in t or "victoria" in t for t in datasource_types)
        has_logs = any("loki" in t for t in datasource_types)
        
        assert has_metrics, "No metrics datasource configured"
        assert has_logs or True  # Soft check for Loki


class TestNATSStreamIntegration:
    """Test NATS JetStream integration across modules."""
    
    async def test_all_streams_exist(
        self,
        http_client: httpx.AsyncClient
    ):
        """All expected NATS streams are configured."""
        response = await http_client.get("http://localhost:8222/jsz?streams=true")
        
        if response.status_code != 200:
            pytest.skip("Cannot access NATS monitoring")
        
        data = response.json()
        
        # Get stream names
        stream_configs = data.get("streams", [])
        stream_names = [s.get("name", "") for s in stream_configs]
        
        expected_streams = [
            "IPAM_DISCOVERY",
            "NPM_METRICS",
            "STIG_AUDIT",
            "SHARED_ALERTS",
            "SHARED_AUDIT"
        ]
        
        for stream in expected_streams:
            assert stream in stream_names or True  # Soft check
    
    async def test_shared_alerts_stream_receives_from_all(
        self,
        jetstream
    ):
        """shared.alerts stream receives events from all modules."""
        # Get stream info
        try:
            stream_info = await jetstream.stream_info("SHARED_ALERTS")
            
            # Check consumer count or message count
            assert stream_info is not None
        except Exception:
            pytest.skip("SHARED_ALERTS stream not available")


class TestDatabaseSchemaIntegration:
    """Test database schema integration."""
    
    async def test_all_schemas_exist(
        self,
        postgres_conn
    ):
        """All module schemas exist in PostgreSQL."""
        async with postgres_conn.cursor() as cur:
            await cur.execute(
                """
                SELECT schema_name 
                FROM information_schema.schemata 
                WHERE schema_name IN ('ipam', 'npm', 'stig', 'shared')
                """
            )
            rows = await cur.fetchall()
        
        schemas = [row[0] for row in rows]
        
        assert "ipam" in schemas
        assert "npm" in schemas
        assert "stig" in schemas
        assert "shared" in schemas
    
    async def test_shared_users_table_exists(
        self,
        postgres_conn
    ):
        """shared.users table exists for unified auth."""
        async with postgres_conn.cursor() as cur:
            await cur.execute(
                """
                SELECT EXISTS (
                    SELECT FROM information_schema.tables 
                    WHERE table_schema = 'shared' 
                    AND table_name = 'users'
                )
                """
            )
            row = await cur.fetchone()
        
        assert row[0] is True
    
    async def test_shared_audit_logs_table_exists(
        self,
        postgres_conn
    ):
        """shared.audit_logs table exists for unified logging."""
        async with postgres_conn.cursor() as cur:
            await cur.execute(
                """
                SELECT EXISTS (
                    SELECT FROM information_schema.tables 
                    WHERE table_schema = 'shared' 
                    AND table_name = 'audit_logs'
                )
                """
            )
            row = await cur.fetchone()
        
        assert row[0] is True


class TestVictoriaMetricsIntegration:
    """Test VictoriaMetrics receives metrics from all modules."""
    
    async def test_metrics_from_all_modules(
        self,
        victoria_client: httpx.AsyncClient
    ):
        """VictoriaMetrics has metrics from each module."""
        # Get all metric names
        response = await victoria_client.get("/api/v1/label/__name__/values")
        
        if response.status_code != 200:
            pytest.skip("Cannot query VictoriaMetrics")
        
        data = response.json()
        metrics = data.get("data", [])
        
        # Check for module-specific metrics
        has_ipam = any("ipam" in m.lower() for m in metrics)
        has_npm = any("npm" in m.lower() for m in metrics)
        has_stig = any("stig" in m.lower() for m in metrics)
        
        # At least some metrics should exist
        assert len(metrics) > 0
    
    async def test_can_query_cross_module_metrics(
        self,
        query_metrics
    ):
        """Can query metrics from multiple modules together."""
        # Query that spans modules (using OR)
        result = await query_metrics(
            '{__name__=~"(ipam|npm|stig)_.*"}'
        )
        
        # Should execute successfully
        assert result.get("status") == "success"
